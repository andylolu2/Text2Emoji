{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text2Emoji ([More info](README.md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up dataset\r\n",
    "1. Remove redundant adjectives from emoji vocabs (E.g. `color` skin tone, flag: `country`)\r\n",
    "    - [Vocab list](processed_data/emoji_vocab.txt)\r\n",
    "2. Get [train](processed_data/training_data-1000000_entries.csv) - [test]((processed_data/testing_data-83280_entries.csv)) data\r\n",
    "    - Filters non-English reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original no. of emojis: 3295\n",
      "cleaned no. of emojis: 1747\n",
      "Longest emoji vocab: South Georgia & South Sandwich Islands\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import re\r\n",
    "\r\n",
    "df = pd.read_csv('data/emojis/emoji_df.csv')\r\n",
    "emoji_vocab = set(df['name'].to_list())\r\n",
    "\r\n",
    "# remove skin tone adjective\r\n",
    "cleaned_emoji_vocab = set()\r\n",
    "for t in emoji_vocab:\r\n",
    "    flag_re = r\"flag: (.*)\"\r\n",
    "    is_flag = re.match(flag_re, t)\r\n",
    "    if is_flag:\r\n",
    "        cleaned_emoji_vocab.add(is_flag[1])\r\n",
    "        continue\r\n",
    "    details_re = r\"(.*): (.*)\"\r\n",
    "    is_detailed = re.match(details_re, t)\r\n",
    "    if is_detailed:\r\n",
    "        # print(f\"{is_detailed[0]}, {is_detailed[1]}, {is_detailed[2]}\")\r\n",
    "        cleaned_emoji_vocab.add(is_detailed[1])\r\n",
    "        continue\r\n",
    "    cleaned_emoji_vocab.add(t)\r\n",
    "\r\n",
    "print(f\"Original no. of emojis: {len(emoji_vocab)}\")\r\n",
    "print(f\"cleaned no. of emojis: {len(cleaned_emoji_vocab)}\")\r\n",
    "print(f\"Longest emoji vocab: {max(cleaned_emoji_vocab, key=lambda t: len(t.split(' ')))}\")\r\n",
    "\r\n",
    "with open('processed_data/emoji_vocab.txt', 'w') as f:\r\n",
    "    for t in sorted(list(cleaned_emoji_vocab)):\r\n",
    "        f.write(f\"{t}\\n\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192414"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "import re\n",
    "\n",
    "\n",
    "sql_conn = sqlite3.connect('data\\\\archive\\\\reddit-comments-may-2015\\\\database.sqlite')\n",
    "# Full dataset has about 54,000,000 entries\n",
    "\n",
    "comments = pd.read_sql(\n",
    "    \"SELECT body FROM May2015 LIMIT(1200000)\", \n",
    "    sql_conn)\n",
    "\n",
    "def filter_fn(row):\n",
    "    length = min(100, len(row['body']))\n",
    "    text = row['body'][:length]\n",
    "    # Filter non-English-dominant posts\n",
    "    return len(re.findall(r\"[\\u0000-\\u007F]\", text)) > length*0.7\n",
    "\n",
    "mask = comments.apply(filter_fn, axis=1)\n",
    "comments = comments[mask]\n",
    "len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "83280\n"
     ]
    }
   ],
   "source": [
    "data = comments.drop_duplicates()\n",
    "training_data = data.iloc[:1_000_000]\n",
    "test_data = data.iloc[1_000_000:]\n",
    "\n",
    "print(len(training_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mini data to csv\r\n",
    "training_data.to_csv(f'processed_data\\\\training_data-{len(training_data)}_entries.csv')\r\n",
    "test_data.to_csv(f'processed_data\\\\testing_data-{len(test_data)}_entries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Note: setnece_tokenizer is same as distilbert_tokenizer\n",
    "sentence_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\")\n",
    "sentence_transformer = AutoModel.from_pretrained(\"sentence-transformers/distilbert-base-nli-stsb-mean-tokens\")\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "dsitilbert = AutoModel.from_pretrained('distilbert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for differences between sentence_tokenizer and distilbert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for differences between sentence_tokenizer and distilbert tokenizer\n",
    "all_same = True\n",
    "for i in range(100):\n",
    "    if sentence_tokenizer.encode(comments['body'].iloc[0]) != distilbert_tokenizer.encode(comments['body'].iloc[0]):\n",
    "        all_same = False\n",
    "\n",
    "assert all_same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921. 100000 soldiers managed to kill only 800 Romans, proper embarrassment\n",
      "\n",
      "1764. I'd rather not say.\n",
      "\n",
      "4208. Awesome. Any specific reason for the power supply and processor picks?\n",
      "\n",
      "7583. BOOOOOO THAT FILTHY SWINE\n",
      "\n",
      "6344. They're/Their/There\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "for i in random.sample(range(len(comments)), 5):\n",
    "    print(f\"{i}. {comments['body'].iloc[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}